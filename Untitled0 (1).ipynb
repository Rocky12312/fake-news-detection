{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQKBlw8hRK1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation,Dropout,Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmlofemTXpMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx-RwWvbaTXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls \"/content/drive/My Drive/fake news detection/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U3ueQFahUdO",
        "colab_type": "text"
      },
      "source": [
        "df,df1,df2,df3 are the dataframes that contain data for fake news prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9md7Xn3VbPdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/fake news detection/data.csv\")\n",
        "df1 = pd.read_csv(\"/content/drive/My Drive/fake news detection/fake.csv\")\n",
        "df2 = pd.read_csv(\"/content/drive/My Drive/fake news detection/fake_or_real_news.csv\")\n",
        "df3 = pd.read_csv('/content/drive/My Drive/fake news detection/train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8mqp60diB7b",
        "colab_type": "text"
      },
      "source": [
        "Bringing all the dataframes in a format such that all the dataframes have same columns. Only keeping the columns which contain the title and text of the news and removing all other from the dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NTbcKQPcAYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(['URLs'], axis = 1, inplace = True)\n",
        "df.loc[df['Label']== 0, 'Label'] = 'REAL'\n",
        "df.loc[df['Label']== 1, 'Label'] = 'FAKE'\n",
        "\n",
        "#joining the title and text columns\n",
        "\n",
        "df[\"matter\"] = df[\"Headline\"].map(str) + df[\"Body\"]\n",
        "df.drop([\"Headline\",\"Body\"],axis = 1,inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJPbG1qrcZSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = df1.loc[df1['type']=='fake']\n",
        "df1.loc[df1['type']== 'fake', 'type'] = 'FAKE'\n",
        "\n",
        "df1.drop(['uuid', 'ord_in_thread', 'author', 'published', \n",
        "       'language', 'crawled', 'site_url', 'country', 'domain_rank',\n",
        "       'thread_title', 'spam_score', 'main_img_url', 'replies_count',\n",
        "       'participants_count', 'likes', 'comments', 'shares'],axis = 1,inplace = True)\n",
        "\n",
        "df1 = df1.rename(columns={'type': 'Label'})\n",
        "\n",
        "#joining the title and text columns\n",
        "\n",
        "df1[\"matter\"] = df1[\"title\"].map(str) + df1[\"text\"]\n",
        "df1.drop([\"title\",\"text\"],axis = 1,inplace = True)\n",
        "df1.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqNNEV4f5tkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2.drop([\"Unnamed: 0\"],axis = 1,inplace = True)\n",
        "\n",
        "df2 = df2.rename(columns = {\"label\":\"Label\"})\n",
        "\n",
        "#joining the title and text columns\n",
        "\n",
        "df2[\"matter\"] = df2[\"title\"].map(str) + df2[\"text\"]\n",
        "df2.drop([\"title\",\"text\"],axis = 1,inplace = True)\n",
        "df2.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZbcC9avjxO-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df3.drop([\"id\",\"author\"],axis = 1,inplace = True)\n",
        "\n",
        "df3 = df3.rename(columns = {\"label\":\"Label\"})\n",
        "\n",
        "#joining the title and text columns\n",
        "\n",
        "df3[\"matter\"] = df3[\"title\"].map(str) + df3[\"text\"]\n",
        "df3.drop([\"title\",\"text\"],axis = 1,inplace = True)\n",
        "\n",
        "df3.loc[df3['Label']== 0, 'Label'] = 'REAL'\n",
        "df3.loc[df3['Label']== 1, 'Label'] = 'FAKE'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aErkkmPo90uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dff = pd.concat([df,df1,df2,df3], ignore_index=True)\n",
        "dff.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k9pbHg7yXEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dropping out the duplicate entries from the dataframe\n",
        "dff = dff.drop_duplicates()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BenfBfAidWZ5",
        "colab_type": "text"
      },
      "source": [
        "some text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRhz9DtxkRpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas.api.types import is_string_dtype\n",
        "is_string_dtype(dff[\"matter\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94tsPKFBnvUZ",
        "colab_type": "text"
      },
      "source": [
        "Lowering the text content and removing punctuations from the data content(matter column)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVTVOb1xqzQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dff[\"matter\"] = dff[\"matter\"].str.lower()\n",
        "dff.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oIs_dx6fZSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "punctuation_to_be_removed = string.punctuation\n",
        "def remove_punctuation(text):\n",
        "  return text.translate(str.maketrans('', '', punctuation_to_be_removed))\n",
        "dff[\"matter_without_punctuation\"] = dff[\"matter\"].astype(str).apply(lambda text: remove_punctuation(text))\n",
        "dff.head()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErBVzrZhzM3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_digits(text):\n",
        "  return ''.join(i for i in text if not i.isdigit())\n",
        "dff[\"matter_without_digits\"] = dff[\"matter_without_punctuation\"].apply(lambda text: remove_digits(text))\n",
        "dff.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1cAbeH3n9oj",
        "colab_type": "text"
      },
      "source": [
        "Removing stopwords from the data content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx8mfcmggP7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
        "dff[\"matter_without_stop_words\"] = dff[\"matter_without_digits\"].apply(lambda text: remove_stopwords(text))\n",
        "dff.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVXn_rMMoej0",
        "colab_type": "text"
      },
      "source": [
        "Removing frequent and rare words from the data content"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7pbqq24mAbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "from collections import Counter\n",
        "count = Counter()\n",
        "for text in dff[\"matter_without_stop_words\"].values:\n",
        "    for word in text.split():\n",
        "        count[word] += 1\n",
        "count.most_common(5)\n",
        "\n",
        "#removing 7 most frequent words\n",
        "\n",
        "frequent_words = set([w for (w, wc) in count.most_common(7)])\n",
        "def remove_frequentwords(text):\n",
        "    \n",
        "    return \" \".join([word for word in str(text).split() if word not in frequent_words])\n",
        "\n",
        "dff[\"matter1\"] = dff[\"matter_without_stop_words\"].apply(lambda text: remove_frequentwords(text))\n",
        "\n",
        "#removing 7 most rare words\n",
        "\n",
        "no_of_rare_words = 7\n",
        "rare_words = set([w for (w, wc) in count.most_common(10)[:-no_of_rare_words-1:-1]])\n",
        "def remove_rarewords(text):\n",
        "    \n",
        "    return \" \".join([word for word in str(text).split() if word not in rare_words])\n",
        "\n",
        "dff[\"matter2\"] = dff[\"matter1\"].apply(lambda text: remove_rarewords(text))\n",
        "dff.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP1I5xLIpwJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download(\"wordnet\")\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words(text):\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "dff[\"final_news_data\"] = dff[\"matter2\"].apply(lambda text: lemmatize_words(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SN9Px2JXUVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9Adsk0LrPx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dff.drop([\"matter\",\"matter_without_punctuation\",\"matter_without_digits\",\"matter_without_stop_words\",\"matter1\",\"matter2\"],axis = 1,inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hJllGCEXWQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg83JfAUopBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maximum_nb_words = 500\n",
        "maximum_sequence_length= 60\n",
        "embedding_dimension = 128\n",
        "tokenizer = Tokenizer(num_words=maximum_nb_words,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(dff[\"final_news_data\"].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('no of unique token %s' % len(word_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIZdmbK2qCO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = tokenizer.texts_to_sequences(dff[\"final_news_data\"].values)\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=maximum_sequence_length)\n",
        "print('Shape of X is:', X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7AXxqZWc6RV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X[3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT8vCS5Pnesn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dff.loc[dff['Label']== \"REAL\", 'Label'] = 0\n",
        "dff.loc[dff['Label']== \"FAKE\", 'Label'] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSa02nGyn7SZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y = dff[\"Label\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODIssO-otBx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Sfffha2-i--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=100)\n",
        "X_train_new = pca.fit_transform(X_train)\n",
        "X_test_new = pca.transform(X_test)\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXPidPrbB0mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_65aDN7A7YJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K5pjNWJAFfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4rmgDr9r7r6",
        "colab_type": "text"
      },
      "source": [
        "Using neural network for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WmDfUveARpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "model = Sequential()\n",
        "model.add(Embedding(maximum_nb_words,embedding_dimension, input_length=X_train.shape[1]))\n",
        "model.add(Dropout(0.6))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, padding='valid', activation='sigmoid',strides = 1))\n",
        "model.add(Dense(256,activation = \"sigmoid\"))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "model.add(Dropout(0.6))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "epochs = 10\n",
        "batch_size = 500\n",
        "model.compile(optimizer = \"adam\",loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
        "history = model.fit(X_train,Y_train, epochs = epochs,\n",
        "                    validation_split = 0.3,batch_size = batch_size,verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6n33Qfyjcx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_learning_curves(history,epochs):\n",
        "  epoch_range = range(1,epochs+1)\n",
        "  #for training and validation accuracy\n",
        "  plt.plot(epoch_range,history.history[\"acc\"])\n",
        "  plt.plot(epoch_range,history.history[\"val_acc\"])\n",
        "  plt.title(\"ACCURACY(model)\")\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.ylabel(\"accuracy\")\n",
        "  plt.legend([\"train\",\"val\"],loc = \"upper left\")\n",
        "  plt.show()\n",
        "  #for training and validation loss\n",
        "  plt.plot(epoch_range,history.history[\"loss\"])\n",
        "  plt.plot(epoch_range,history.history[\"val_loss\"])\n",
        "  plt.title(\"LOSS(model)\")\n",
        "  plt.xlabel(\"epochs\")\n",
        "  plt.ylabel(\"loss\")\n",
        "  plt.legend([\"train\",\"val\"],loc = \"upper left\")\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q3q3jxUj58l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_learning_curves(history,epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omAK10TrkDws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss,test_acc = model.evaluate(X_test,Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBym39GHk1pH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVr9KXd9mZo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ato2aTvzmaBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd6Dav7tmaKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}